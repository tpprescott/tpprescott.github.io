---
layout: slides
title: WMBC Group Meeting MT20 Wk8
---

<section>
	<h2>Bayesian Synthetic Likelihood</h2>
	<h4>WCMB Group Meeting<br>
		MT20 Wk8<br>
		Methods for Parameter Estimation</h4>
	<p>Tom Prescott</p>
</section>

<section>
	<h3>Recap: Bayesian Inference</h3>
	<section>
		<h4>Inputs</h4>
		<p class="fragment fade-in-then-semi-out">
			A <b>model</b> of how a parameterised real-world system gives rise to measurable data.
		</p>
		<p class="fragment fade-in-then-semi-out">
			Prior distribution on a parameter space:
			that is, a density function, $\pi(\theta)$, defined for all $\theta \in \Theta$.
		</p>
		<p class="fragment fade-in-then-semi-out">
			Observation(s) in a data space: $\mathrm{x}_{\mathrm{obs}}$
		</p>
	</section>
	<section>
		<h4>Bayesian computation</h4>
		<p>What is the probability density of observing the data, $\mathrm{x}_{\mathrm{obs}}$, as a function of the parameter value?
			\[ L(\theta) = p(\mathrm{x}_{\mathrm{obs}} ~|~ \theta) \]
			This is where the <b>model</b> comes in, as $p$.
		</p>
	</section>
	<section>
		<h4>Output</h4>
		<p>
			Posterior distribution, with density,
			\[ p(\theta~|~\mathrm{x}_{\mathrm{obs}})
				\propto L(\theta) \pi(\theta), \]
			defined on the parameter space, $\theta \in \Theta$.
		</p>
	</section>
</section>

<section>
	<h3>Example: Coin tossing</h3>
	<section>
		<h4>Inputs</h4>
		<p class="fragment fade-in-then-semi-out">Model parametrised by the probability, $\theta \in [0,1]$, of tossing a head, $H$.</p>
		<p class="fragment fade-in-then-semi-out">Uniform prior: $\pi(\theta) = 1$.</p>
		<p class="fragment fade-in-then-semi-out">Data: $x_{\mathrm{obs}} = (H,T)$.</p>
	</section>
	<section>
		<h4>Bayesian computation</h4>
		<p>
			The likelihood function is binomial:
			\[ L(\theta) = \binom{H+T}{H} \theta^H (1-\theta)^T \]
		</p>
	</section>
	<section>
		<h4>Output</h4>
		<p>
			The posterior is proportional to the product of prior and likelihood:
			\[ \pi(\theta~|~H,T) \propto \binom{H+T}{H} \theta^H (1-\theta)^T \times 1 \]
		</p>
	</section>
	<section>
		<p>
			\[ \pi(\theta~|~H,T) \propto \binom{H+T}{H} \theta^H (1-\theta)^T \times 1 \]
		</p>
		<img src="bayes.gif">
	</section>
</section>


<section>
	<h3>Likelihood-free Bayes</h3>
		<section>
			<h4>Bayesian computation</h4>
			<p>What is the probability density of observing the data, $\mathrm{x}_{\mathrm{obs}}$, as a function of the parameter value?
				\[ L(\theta) = p(\mathrm{x}_{\mathrm{obs}} ~|~ \theta) \]
			</p>
			<p class="fragment"><em>Is this always a reasonable question?</em></p>
		</section>
		<section>
			<iframe width="100%" height="450" data-src="https://www.youtube-nocookie.com/embed/prSjVaATx0g?mute=1&autoplay=1&loop=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		</section>
		<section>
			<iframe width="100%" height="450" data-src="https://www.youtube-nocookie.com/embed/hz7UjN_vYuw?start=117&mute=1&autoplay=1&loop=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		</section>
		<section>
			<h4>Approximating the likelihood</h4>
			<p class="fragment fade-in-then-semi-out">Assume that the likelihood is unavailable, either analytically or computationally</p>
			<p class="fragment fade-in">
				 Then, we need to work with an approximation:
				\[ \hat L(\theta) \approx p(\mathrm{x}_{\mathrm{obs}} ~|~ \theta) \]
			</p>
		</section>
</section>

<section>
	<h3>Synthetic Likelihood</h3>
	<section data-auto-animate>
		<h4>Example of <b>simulation-based inference</b></h4>
		<p class="fragment fade-in-then-semi-out">
			Assume that we can simulate the model under any $\theta \in \Theta$:
			that is, we can draw a random $x \sim p(\cdot ~|~ \theta)$.
		</p>
		<p class="fragment fade-in-then-semi-out">
			But we cannot calculate $p(x ~|~ \theta)$.
		</p>
		<p class="fragment fade-in">
			<em>Given $\theta$, I can simulate flipping $N$ coins and say how many were $H$.</em>
		</p>
		<p class="fragment fade-in">
			<em>But I can't tell you how likely you were to get $H$ heads (and especially $H_{\mathrm{obs}}$ heads) from $N$ flips.</em>
		</p>
	</section>
	<section data-auto-animate>
		<p>
			<em>Given $\theta$, I can simulate flipping $N$ coins and say how many were $H$.</em>
		</p>
		<p class="fragment">
			And I can do this $M$ times, producing a sample:
		</p>
		<p class="fragment">
			\[ H_1, \dots, H_M \sim p(\cdot~|~\theta). \]
		</p>
		<p class="fragment">
			The idea of <b>synthetic likelihood</b> is to fit
			a Normal distribution to this empirical sample.
		</p>
	</section>
	<section data-auto-animate>
		<p>
			\[ H_1, \dots, H_M \sim p(\cdot~|~\theta). \]
		</p>
		<p class="fragment">
			We fit a Normal distribution $\phi(\cdot~|~\theta)$ with mean and variance,
			\[
			\mu(\theta) = \mathrm{mean}(H_1, \dots, H_M) \\
			\Sigma^2(\theta) = \mathrm{var}(H_1, \dots, H_M)
			\]
		</p>
	</section>
	<section>
		<p>
			The synthetic likelihood of the observed data, $H_{\mathrm{obs}}$,
			is the likelihood under this empirically fit Normal distribution:
			\[ L(\theta) \approx \hat L(\theta) = \phi(H_{\mathrm{obs}}~|~\theta) \]
		</p>
	</section>
</section>

<section>
	<h3>Summary</h3>
	<p class="fragment">1. Simulate $M$ times and get $x_i \sim p(\cdot~|~\theta)$.</p>
	<p class="fragment">2. Fit $\mu(\theta) = \mathrm{mean}(x_i)$.</p>
	<p class="fragment">3. Fit $\Sigma^2(\theta) = \mathrm{cov}(x_i)$.</p>
	<p class="fragment">4. Calculate synthetic likelihood,
		\[ \hat L(\theta) = \phi(x_{\mathrm{obs}}~|~\mu(\theta), \Sigma^2(\theta)) \]
	</p>
</section>

<section>
	<h3>Pitfalls</h3>
	<p>Data dimension</p>
	<p>Computational expense</p>
	<p>Normality assumption</p>
</section>

<section>
	<h3>Data dimension</h3>
	<section>
		<h4>High-dimensional data needs summarising</h4>
		<img src="NoEF_TrajObs.png" width=350>
	</section>
	<section>
		<h4>Summary statistics</h4>
		<p>"Throw away" information: make data and simulations just 4D.</p>
		<p class="fragment">1. Final displacement from origin.</p>
		<p class="fragment">2. Final angle made with positive $x$-axis.</p>
		<p class="fragment">3. Average displacement between sample times.</p>
		<p class="fragment">4. Std. dev. of displacement between sample times.</p>
	</section>
	<section>
		<h4>Summary statistics - results</h4>
		<img src="both.png" width=800>
	</section>
</section>

<section>
	<h3>Very selected reading</h3>
	<p>Price et al (2018) "Bayesian Synthetic Likelihood." <a href="https://doi.org/10.1080/10618600.2017.1302882">DOI.</a></p>
	<p>Priddle et al (2019) "Efficient Bayesian synthetic likelihood with whitening transformations." <a href="https://arxiv.org/abs/1909.04857">arXiv.</a></p>
	<p>Picchini et al (2020) "Adaptive MCMC for synthetic likelihoods and correlated synthetic likelihoods." <a href="https://arxiv.org/pdf/2004.04558.pdf">arXiv.</a></p>
</section>
