---
layout: slides
title: CMStatistics 2020
---

<section>
	<h3>Multifidelity Approximate Bayesian Computation</h3>
	<h4>
		<i>Thomas Prescott</i> and Ruth Baker<br>
		University of Oxford
	</h4>
	<p>
		CMStatistics 2020<br>
		KCL / Zoom: 19 December 2020
	</p>
</section>

<section>
	<h3>Reading</h3>
	<p>
		Prescott and Baker.
		<a href="https://doi.org/10.1137/18M1229742">Multifidelity approximate Bayesian computation.</a>
		SIAM/ASA Journal of Uncertainty Quantification 8:114 (2020)
	</p>
	<p>
		Prescott and Baker.
		<a href="https://arxiv.org/abs/2001.06256">Multifidelity approximate Bayesian computation with sequential Monte Carlo parameter sampling.</a>
		arXiv:2001.06256 (2020)
</section>

<section>
	<h3>Abstract</h3>
	<section><ul>
		<li class="fragment highlight-current-green">Simulation-based, likelihood-free Bayesian inference is expensive.</li>
		<li class="fragment highlight-current-green">We can use <em>low-fidelity approximations</em> of high-fidelity models...</li>
		<li class="fragment highlight-green">...but this introduces uncontrolled bias.</li>
	</ul></section>
	<section><ul>
		<li>We have removed this bias with a <em>multifidelity</em> approach:
			<ul>
				<li class="fragment highlight-current-green">Use low-fidelity simulations as much as possible.</li>
				<li class="fragment highlight-current-green">Correct with high-fidelity simulations when appropriate.</li>
			</ul>
		</li>
		<li class="fragment highlight-green">Multifidelity simulation can be tuned to minimise sample variance for a given simulation budget.</li>
	</ul></section>
</section>

<section data-auto-animate>
	<h3>Bayesian inference</h3>
	<p>
		Map prior, $\pi(\theta)$, to posterior, $\pi(\theta~|~x_{\mathrm{obs}})$ using Bayes's rule:
	</p>
	<p data-id="Bayes">
		$\pi(\theta~|~ x_{\mathrm{obs}}) \propto L(\theta) \pi(\theta).$
	</p>
	<p class="fragment">
		Here, $L(\theta) = f(x_{\mathrm{obs}}~|~\theta)$ is the <em>likelihood</em>
		of observed data, $x_{\mathrm{obs}}$,
		for a given parameter value, $\theta$,
		under a given model, $f$.
	</p>
	<p class="fragment">
		The likelihood, $L(\theta)$, often cannot be calculated.
	</p>
</section>

<section data-auto-animate>
	<h3>Likelihood-free Bayes</h3>
	<p data-id="Bayes">
		$\pi(\theta~|~ x_{\mathrm{obs}}) \propto L(\theta) \pi(\theta)$.
	</p>
	<p class="fragment">
		Approximate $L(\theta) \approx \hat L(\theta)$ with a function that can be calculated,
		or estimated, based on model simulations:
		<ul class="fragment">
			<li>Synthetic likelihoods</li>
			<li class="fragment highlight-green">Approximate Bayesian computation (ABC)</li>
		</ul>
	</p>
</section>

<section data-auto-animate>
	<h3>ABC likelihood</h3>
	<ul>
		<li class="fragment">Denote simulations of the parametrised model, \[ y \sim f(\cdot~|~\theta). \]</li>
		<li class="fragment">Define a <em>neighbourhood</em> of data, \[ \Omega = \{ y~:~d(y,x_{\mathrm{obs}})<\epsilon \} \] for some distance, $d$, and threshold, $\epsilon$.</li>
		<li class="fragment" data-id="L_ABC">Approximate the likelihood, \[ \hat L_{\mathrm{ABC}}(\theta) = \mathbf P(y \in \Omega~|~\theta). \]</li>
	</ul>
</section>

<section data-auto-animate>
	<h3>ABC posterior</h3>
	<ul>
		<li data-id="L_ABC">Approximate the likelihood, \[ \hat L_{\mathrm{ABC}}(\theta) = \mathbf P(y \in \Omega~|~\theta). \]</li>
		<li class="fragment">This implies an approximate posterior:	\[ \pi_{\mathrm{ABC}}(\theta~|~x_{\mathrm{obs}}) \propto \mathbf P(y \in \Omega~|~\theta) \pi(\theta). \]</li>
	</ul>
</section>


<section>
	<h3>Discrete parameter example</h3>
	<section>
		<p>Stochastic time, $y$ seconds, for $\theta$ enzyme molecules to convert 100 substrate molecules into product.</p>
		<div class="r-stack">
			<span class="fragment fade-in" data-fragment-index=1>
				<span class="fragment fade-out" data-fragment-index=2>
					<img src="hifi_f6.png" height="280">
				</span>
			</span>
			<span class="fragment fade-in" data-fragment-index=2>
				<span class="fragment fade-out" data-fragment-index=3>
					<img src="hifi_f.png" height="280">
				</span>
			</span>
			<img class="fragment fade-in" data-fragment-index=3 src="hifi_pi.png" height="280">
		</div>
		<p class="fragment" data-fragment-index=1>Observed data, $x_{\mathrm{obs}}=180$ with threshold $\epsilon=10$.</p>
	</section>
</section>

<section>
	<h3>Low-fidelity models</h3>
	<p class="fragment fade-in-then-semi-out">
		The computational burden of likelihood-free inference is dominated by simulation.
	</p>
	<p class="fragment fade-in-then-semi-out">
		Many fields aim to speed simulation with cheaper, approximate, <em>low-fidelity</em> models.
	</p>
	<p class="fragment">
		Examples include space/time discretisation, model order reduction, timescale separation, early termination, ODE and mean field approximations.
	</p>
</section>

<section>
	<h3>Discrete parameter example</h3>
	<section>
		<p>Inference using a <em>low-fidelity</em> model, again with observed data, $x_{\mathrm{obs}}=180$ with threshold $\epsilon=10$.</p>
		<div class="r-stack">
			<span class="fragment fade-in" data-fragment-index=1>
				<span class="fragment fade-out" data-fragment-index=2>
					<img src="lofi_f6.png" height="280">
				</span>
			</span>
			<span class="fragment fade-in" data-fragment-index=2>
				<span class="fragment fade-out" data-fragment-index=3>
					<img src="lofi_f.png" height="280">
				</span>
			</span>
			<img class="fragment fade-in" data-fragment-index=3 src="lofi_pi.png" height="280">
		</div>
	</section>
</section>

<section>
	<h3>Comparing fidelities</h3>
	<section>
		<div class="r-stack">
			<img class="fragment fade-out" data-fragment-index=1 src="compare_f.png" height="500">
			<img class="fragment fade-in" data-fragment-index=1 src="compare_pi.png" height="400">
		</div>
	</section>
</section>

<section data-transition="fade">
	<h3>Question</h3>
	<p>Can we use low-fidelity simulations to calibrate the high-fidelity model?</p>
</section>

<section data-transition="fade">
	<h3>Yes, but...</h3>
	<p class="fragment">... not on their own.</p>
</section>

<section>
	<h3>ABC weighting</h3>
	<section>
		<p>ABC likelihood 'works' by Monte Carlo estimation.</p>
		<p class="fragment">
			Given $\theta$, simulate $y \sim f(\cdot~|~\theta)$ and calculate a weight,
			\[ w(\theta, y) = \mathbf 1(y \in \Omega). \]
		</p>
		<p class="fragment">
			The key is the conditional expectation:
			\[ \mathbf E(w~|~\theta) = \mathbf P(y \in \Omega~|~\theta) = \hat L_{\mathrm{ABC}}(\theta). \]
		</p>
	</section>
	<section>
		<p>Low-fidelity inference 'fails' because we apply a different weight,
			\[ \tilde w(\theta, \tilde y) = \mathbf 1(\tilde y \in \Omega), \]
		</p>
		<p class="fragment">
			with the 'wrong' conditional expectation,
			\[ \mathbf E(\tilde w~|~\theta) = \mathbf P(\tilde y \in \Omega~|~\theta) \neq \hat L_{\mathrm{ABC}}(\theta). \]
		</p>
	</section>
	<section>
		<p>
			Each weight, $w$ and $\tilde w$, incurs simulation costs, $T(\theta, y)$ and $\tilde T(\theta, \tilde y)$, respectively.
		</p>
		<p class="fragment">
			Key assumption is that low-fidelity simulation is <em>significantly cheaper</em> than high-fidelity:
			\[ \mathbf E(\tilde T~|~ \theta) \ll \mathbf E(T~|~\theta). \]
		</p>
	</section>
</section>

<section data-auto-animate>
	<h3>Multifidelity weighting</h3>
	<p>Task: design a cheap, unbiased weight:</p>
	<p data-id="w_mf">
		<a>$w_{\mathrm{mf}}(\theta, $</a>
		<a class="fragment" data-fragment-index=1>$\tilde y,$</a>
		<a class="fragment" data-fragment-index=2>$y,$</a>
		<a class="fragment" data-fragment-index=3>$u$</a>
		<a>$)~= $</a>
		<br>
		<a class="fragment" data-fragment-index=1>$\mathbf 1(\tilde y \in \Omega)$</a>
		<a class="fragment" data-fragment-index=2>$+ \left[ \mathbf 1(y \in \Omega) - \mathbf 1(\tilde y \in \Omega) \right]$</a>
		<a class="fragment" data-fragment-index=3>$\mathbf 1(u < \alpha)$</a>
		<a class="fragment" data-fragment-index=4>$\frac{1}{\alpha}$</a>
	</p>
	<p>
		<span class="fragment highlight-current-red" data-fragment-index=1>
		<span class="fragment highlight-current-green" data-fragment-index=2>
		<span class="fragment highlight-current-red" data-fragment-index=3>
		<span class="fragment highlight-green" data-fragment-index=4>
			Conditional expectation:
		</span></span></span></span>
		<span class="r-stack">
			<a class="fragment fade-in-then-out" data-fragment-index=1>$\mathbf P(\tilde y \in \Omega~|~\theta)$</a>
			<a class="fragment fade-in-then-out" data-fragment-index=2>$\mathbf P(y \in \Omega~|~\theta)$</a>
			<a class="fragment fade-in-then-out" data-fragment-index=3>$\alpha \mathbf P(y \in \Omega~|~\theta) + (1-\alpha) \mathbf P(\tilde y \in \Omega~|~\theta)$</a>
			<a class="fragment fade-in" data-fragment-index=4>$\mathbf P(y \in \Omega~|~\theta)$</a>
		</span>
	</p>
	<p>
		<span class="fragment highlight-current-green" data-fragment-index=1>
		<span class="fragment highlight-current-red" data-fragment-index=2>
		<span class="fragment highlight-green" data-fragment-index=3>
			Simulation cost:
		</span></span></span>
		<span class="r-stack">
			<a class="fragment fade-in-then-out" data-fragment-index=1>$\mathbf E(\tilde T~|~\theta)$</a>
			<a class="fragment fade-in-then-out" data-fragment-index=2>$\mathbf E(\tilde T~|~\theta) + \mathbf E(T~|~\theta)$</a>
			<a class="fragment fade-in-then-out" data-fragment-index=3>$\mathbf E(\tilde T~|~\theta) + \alpha \mathbf E(T~|~\theta)$</a>
			<a class="fragment fade-in" data-fragment-index=4>$\mathbf E(\tilde T~|~\theta) + \alpha \mathbf E(T~|~\theta)$</a>
		</span>
	</p>
</section>
<section data-auto-animate>
	<h3>Multifidelity weighting</h3>
	<p data-id="w_mf">
		<a>$w_{\mathrm{mf}}(\theta, \tilde y, y, u) = $</a>
		<br>
		<a>$\mathbf 1(\tilde y \in \Omega) + \left[ \mathbf 1(y \in \Omega) - \mathbf 1(\tilde y \in \Omega) \right] \mathbf 1(u < \alpha) \frac{1}{\alpha} $</a>
	</p>
	<ul>
		<li>Given $\theta$,
		<ul class="fragment">
			<li>Simulate low-fidelity $\tilde y \sim \tilde f(\cdot~|~\theta)$.</li>
			<li>Set $w_{\mathrm{mf}} = \mathbf 1(\tilde y \in \Omega)$.</li>
			<li class="fragment">With probability $\alpha$, <em>correct</em>:
				<ul class="fragment">
					<li>Simulate high-fidelity $y \sim f(\cdot~|~\theta)$.</li>
					<li>Update $w_{\mathrm{mf}} \leftarrow w_{\mathrm{mf}} + (\mathbf 1(y \in \Omega) - w_{\mathrm{mf}})/\alpha$.</li>
				</ul>
			</li>
			<li class="fragment">Return $w_{\mathrm{mf}}$.</li>
		</ul></li>
	</ul>
</section>

<section>
	<h3>The catch...?</h3>
	The multifidelity weight, $w_{\mathrm{mf}}(\theta, \tilde y, y, u)$, is:
	<p><ul>
		<li>Unbiased: \[\mathbf E(w_{\mathrm{mf}}~|~\theta) = \mathbf P(y \in \Omega~|~\theta). \]</li>
		<li>Cheap: \[ \mathbf E(T_{\mathrm{mf}}~|~\theta) = \mathbf E(\tilde T~|~\theta) + \alpha \mathbf E(T~|~\theta). \] </li>
	</ul></p>
</section>

<section>
	<h3>The catch!</h3>
	<section data-auto-animate>
		<h4>Variance</h4>
		<p class="fragment">With $\alpha < 1$, any <em>disagreement</em> between the models increases the variance of $w_{\mathrm{mf}}$:</p>
		<p class="fragment" data-id="variance">
			\[ \begin{align} \mathrm{Var} (w_{\mathrm{mf}}~|~\theta) &= \mathrm{Var} (w~|~\theta) \\
			&+ \left(\frac{1}{\alpha} - 1\right) \mathbf P(\mathrm{disagree}~|~\theta) . \end{align} \]
		</p>
	</section>
	<section data-auto-animate>
		<p data-id="variance">
			\[ \begin{align} \mathrm{Var} (w_{\mathrm{mf}}~|~\theta) &= \mathrm{Var} (w~|~\theta) \\
				&+ \left(\frac{1}{\alpha} - 1\right) \mathbf P(\mathrm{disagree}~|~\theta) . \end{align} \]
		</p>
		<span class="r-stack">
			<span class="r-stretch">
				<span class="fragment fade-in-then-out">
					<img src="lofi_f6.png" height="270">
					<img src="hifi_f6.png" height="270">
				</span>
			</span>
			<img class="fragment fade-in" src="pairedsims_6.png" height="350">
		</span>
	</section>
</section>

<section data-auto-animate>
	<h3>Tuning $\alpha$</h3>
	<p>Decrease $\alpha$ to trade off <em>decreased</em> simulation time,
		\[ \mathbf E(T_{\mathrm{mf}}~|~\theta) = \mathbf E(\tilde T~|~\theta) + \alpha \mathbf E(T~|~\theta), \]
		against <em>increased</em> variance,</p>
		<p data-id="variance">\[ \begin{align} \mathrm{Var} (w_{\mathrm{mf}}~|~\theta) &= \mathrm{Var} (w~|~\theta) \\
			&+ \left(\frac{1}{\alpha} - 1\right) \mathbf P(\mathrm{disagree}~|~\theta) . \end{align} \]</p>
</section>

<section data-auto-animate>
	<h3>Optimal continuation probabilities</h3>
	<section data-auto-animate>
		<p>Reciprocal relationship between MC estimator variance and total simulation cost:</p>
		<p>
			$\mathrm{Var} \left( \frac{\sum w_{\mathrm{mf},i} F(\theta_i)}{\sum w_{\mathrm{mf},i}} \right)\mathbf E(T_{\mathrm{tot}}) = \phi(\alpha(\theta))$
		</p>
		<ul>
			<li class="fragment">Analytical expression for functional $\phi(\alpha)$ exists.</li>
			<li class="fragment">Can choose function $\alpha^\star (\theta) \in (0,1]$ to minimise $\phi$.</li>
		</ul>
	</section>
	<section data-auto-animate>
		<p data-id="alpha_star">
			$\alpha^\star (\theta) = \frac{1}{\lambda} |F(\theta) - \mathbf E(F)| \left( \frac{\mathbf E(\tilde T)}{\mathbf E(T~|~\theta)} \mathbf P(\mathrm{disagree}~|~\theta)\right)^{1/2} $
		</p>
		<ul>
			<li class="fragment">Common factor, $\lambda$, decreases as the average accuracy of the low-fidelity model decreases.</li>
			<li class="fragment">Expensive high-fidelity simulations decrease $\alpha$.</li>
			<li class="fragment">Disagreement between fidelities increases $\alpha$.</li>
		</ul>
	</section>
</section>

<section data-auto-animate>
	<h3>Discrete parameter example</h3>
	<p data-id="alpha_star">
		$\alpha^\star (\theta) = \frac{1}{\lambda} |F(\theta) - \mathbf E(F)| \left( \frac{\mathbf E(\tilde T)}{\mathbf E(T~|~\theta)} \mathbf P(\mathrm{disagree}~|~\theta)\right)^{1/2} $
	</p>
	<span class="r-stack">
		<img class="r-stretch fragment fade-out" data-fragment-index=1 src="alpha_star.png">
		<p class="fragment fade-in" data-fragment-index=1>Calculated to minimise the estimator variance of $\pi_{\mathrm{ABC}}(6~|~x_{\mathrm{obs}})$.</p>
	</span>
</section>

<section>
	<img class="r-stretch" src="mf_pi6.png">
	<span class="r-stack">
		<p class = "fragment fade-out" data-fragment-index=1>
			We have <em>somewhat</em> reduced the variance of the MC estimate of
			$\pi_{\mathrm{ABC}}(6~|~x_{\mathrm{obs}})$.
		</p>
		<p class = "fragment fade-in" data-fragment-index=1>
			But there is scope for much more improvement!
		</p>
	</span>
</section>
<section>
	<span class="r-stack">
		<h4 class="fragment fade-out" data-fragment-index=2>"With probability $\alpha(\theta)$, simulate hi-fi..."</h4>
		<h4 class="fragment fade-in" data-fragment-index=2>"With probability $\alpha(\theta, \tilde y)$, simulate hi-fi..."</h4>
	</span>
	<img class="fragment fade-in" data-fragment-index=1 src="pairedsims_6.png" height="400">
</section>

<section>
	<h3>Ongoing work</h3>
		<ul>
			<li class="fragment highlight-current-green">Learning optimal continuation probabilities as a function of (continuous) parameter and low-fidelity output:
				$\alpha(\theta, \tilde y)$.</li>
			<li class="fragment highlight-current-green">More than one low-fidelity model with accuracy and savings targeted to different regions of parameter space.</li>
			<li class="fragment highlight-green">Adaptively improving low-fidelity models and couplings, to reduce disagreement probabilities.</li>
		</ul>
</section>

<section>
	<h3>Reading</h3>
	<section>
		<p>
			Prescott and Baker.
			<a href="https://doi.org/10.1137/18M1229742">Multifidelity approximate Bayesian computation.</a>
			SIAM/ASA Journal of Uncertainty Quantification 8:114 (2020)
		</p>
		<p>
			Prescott and Baker.
			<a href="https://arxiv.org/abs/2001.06256">Multifidelity approximate Bayesian computation with sequential Monte Carlo parameter sampling.</a>
			arXiv:2001.06256 (2020)
	</section>
	<section>
		<ul>
			<li class="fragment highlight-current-green">
				Continuous, high-dimensional parameter spaces.
			</li>
			<li class="fragment highlight-current-green">
				Reducing disagreement by coupling simulations.
			</li>
			<li class="fragment highlight-current-green">
				Early acceptance and early rejection continuation probabilities:
				$\alpha(\tilde y) = \eta_1 \mathbf 1(\tilde y \in \Omega) + \eta_2 \mathbf 1(\tilde y \notin \Omega)$.</li>
			<li class="fragment highlight-green">
				Exploiting SMC to estimate key quantities required for tuning $\alpha$, including
				simulation times, $\mathbf E(\tilde T)$ and $\mathbf E(T)$, and
				disagreement probabilities, $\mathbf P(\mathrm{disagree})$.
			</li>
		</ul>
	</section>
</section>



<section>
	<h3>Thank you!</h3>
	<p>I'm happy to take questions.</p>
	<a href="https://tppres.co.tt">tppres.co.tt</a><br><a href="https://twitter.com/tpprescott">@tpprescott</a>
	<p>With thanks to <a href="http://www.iamruthbaker.com">Ruth Baker</a> and UKRI Biotechnology and Biological Sciences Research Council project BB/R000816/1.</p>
</section>
